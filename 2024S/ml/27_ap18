NEURAL NETWORKS:

Practical Considerations:
  NO GUARANTEE OF CONVERGENCE
    may oscillate or reach a local minimum
  In practice, many large networks are trained on LARGE AMOUNTS OF DATA for realizstic problems
  Many epochs (sometimes thousands) may be needed for adequate training
    -Large datasets may require hours/day/weeks of CPU/GPU time
    -Sometimes even specialized hardware just to improve efficency of working with NNs
  TERMINATION CRITERIA:
    Number of epochs
    Threshold on training set error
    No decrease in error
    Increased error on validation set
  To avoid local minima: 
    several trials with different random initial weights with majority or voting techniques

MINIBATCHES:
  Stochastic gradient descent:
    take a random example at each step
    write down the loss function with that example
    compute gradient of this loss 
    take a step
  But why take only one at each step?

  Stochastic gradient descent with minibatches
    happy medium between stochastic gradient descent and original gradient descent

    Collect a small number of random examples (the minibatch) at each step
    Write down the loass function with respect to those examples
    Compute the gradient of the loss and take a step

  New hyperparameter: batch size
    often governs how fast the learning converges
    Hardware considerations about memory: want as big a batch as your hardware can accomodate in parallel

GRADIENT TRICKS
  Simple gradient descent updates the parameters using the gradient of one example (or a minibatch) g_i

  step:
    paramters = parameters - learning_rate * g_i

  Gradients could change much faster in one direction than another
    When gradients change very fast in one direction, this could make learning slow or even unstable.
    The quality of the model could change drastically based on how many epochs you run.

  MOMENTUM:
    instead of updataing with the gradient g_i, use a moving average of gradients v_t to update the model parameters.
    v_{t+1} = mu v_t + learning_rate_t g_i
    paramters = parameters - v_t

    New hyperparameter mu controls how much of the previous update should retained. Typically mu=0.9

    Smooths out the updates by using a weighted average of previous gradients at each step


  AdaGrad, RMSProp, Adam
    AdaGrad:
      short for "adaptive gradient"
      Each parameter has its own learning rate. If g_i,t is the gradient for the ith parameter at step t, 
      c_i = c_i + (g_i,t)^2
      parameters_i = parameters_i - (learning_rate)/(alpha + sqrt(c_i)) * g_i,t

    RMSprop:
      similar to AdaGrad, but more recent gradients are weighted more in the denominator:
        c_i = Delta c_i + (1 - Delta) (g_{i,t})^2

    Adam:
      Most common ly used today
      Combines many ideas:
        momentum to smooth gradients
        rmsprop like approach to adaptively choose learning rate with more recent gradients weighted higher
        additional terms to avoid bias introduced during early gradient estimates

  PREVENTING OVERFITTING
    running too many epochs may over-train the network and result in overfitting

    regularization doesn't really work as well for large NNs
    !Keep a held-out validation set and test accuracy after every epoch

    Maintain weights for best performing network on the validation set and return it when performance decreases significantly beyond that

    DROPOUT TRAINING
      During training, for each step, decide whether to delete a hidden unit with some probability p
      That is, make predictions using only a randomly choset set of neurons
      Update only those neurons.
      Tends to avoid overfitting
      Has a model averaging effect: Only some parameters get trained at any step

    Number of hidden units:
      too few  -> prevent the system from adequately fitting the data and learning the concept
      too many -> leads to overfitting
      Cross validation or performance on a held out set can be used to determine the appropriate number







GENERATIE VS DISCRIMINATIVE LEARNING
  WHat we saw most of the semester:
    A fixed, unknown distribution D over X-> Y

    Learning: idnetify a hypothesis space, define a loss function, minimize average loss over training data

    Guarantee: if we . . . .

    (Discriminvative modedl)

  GENERATIVE MODELS:
    Explicitly model how instances in each category are generated by modeling the joint probability of X and Y, that is P(Y,X)

    That is learn P(X|Y) and P(Y)

    The naiive bayes classifier does this.
    Naiive bayes is a generative model.
    Predict P(Y|X) using the bayes rule.

    Big difference is that Generative Models explicitly characterise imputs and outputs

    Generative Models: 
      learn P(x, y)
      Use the capacity of the model to characterize how the data is generated (both inputs and outputs)
      eg naiive bayes, hidden markov model

    Discriminative models
      learn P(y | x)
      Use model capacity to characterize the decision boundary only
      Eg logistic regression, conditional models, most neural models