HW6 is now available, due April 23rd

Final project due on April 23rd, no late penalty until April 30th ðŸ˜‰
  Because the deadline can't be on or after finals

Final on April 25th, 1030-1230



BAYESIAN LEARNING

Two examples of maximum likelihood estimation

Bayes' Rule: P(h|D) \proportional P(h) P(D|h)


What we need in order to define learning:
  A hypothesis space H
  A model that says how data is generated from H

  suppose H consists of real valued functions
    inputs are vectors x \in Real ^ d and the output is a real number y \in Real

  Suppose the training data is generated as follows:
    an input x_i is drawn randomly (uniformly at random)
    the true function f is applied to get f(x_i)
    The value is then perturbed by noise e_i ~ (some unknown gaussian with zero mean)
    y_i = f(x_i) + e_i

  Suppose we have a hyptohesis h. We want to know what is the probability that a particular label y_i was generated by this hypothesis as h(x_i)
  the error for this example is y_i - h(x_i)

  So we assume that this error is from a gaussian distribution
  we can ask, what is the probability of observing one data point (x_i, y_i) if it were generated during the function h
    (just plug into gaussian formula)

  Since each data point is independent, we can just multiply these together to arrive at likilihood of full dataset.

  *** see lecture slides to see how to simplify gaussian formula ***

  Gaussian is equivalent to h_ML = argmin_{h \in H} sum_{i=1}^{m}{(y_i - h(x_i)) ^2}

  if we consider H = set of linear functions, h(x_i) = w dot x_i, so this evaluates to the same as Least Squares regression 
    probabilistic interpretation of least mean squares regression
    tied to because we assumed noise is gaussian

Linear Regression: Two perspectives
  Loss minimization perspective: we want to minimize the difference between the squared loss error of our prediction
  Bayesian: We believe that the errors are normally distributed with zero mean and fixed variance. Find the linear regressor using the maximum likelihood principle.

Summary: 

    Bayesian Learning is another way to ask what is the best hypothesis for a dataset
    Two answers to this question: Maximum A Posteriori, Maximum Likelihood
    We saw two examples, we should be able to apply MAP and MLE to simple problems



LOGISTIC REGRESSION

  We have seen:
    linear models
    learning as loss minimization
    Bayesian learning criteria (MAP and MLE)


  Logistic Regression:

    The setting:
      Binary classification
      Inputs x \in Real^d
      Labels y \in {-1, +1}

    Training data:
      S = {(x_i, y_i)}

    The output y is discrete: Either -1 or +1
    BUT, instead of predicting a label, let us try to predict P(y_i  = +1 | x_i )

    That is, 
    Expand  hypothesis to functions whose output is in the range [0,1]
    Originally, Real^d -> {-1, +1}
    Modified,   Real^d -> [0,1]
    Effectively, make the problem a regression problem


    Sigmoid function: the hypothesis space for logistic regression

      All functions of the form h_w(x) = sigma(w dot x) = 1 / (1 + exp(-w dot x)) 

      Derivative of sigmoid at z is sigmoid(z) (1 - sigmoid(z))
 
      P(y | x  ; w) = 1 / (1 + exp(-y w dot x))
      
      Return 1 if probability of y=+1 is half or greater, otherwise predict -1
      
      this rearranges to predict sgn(w dot x)
      



Training a logistic regression classifier

  On Maximum Likelihood estimator
  
    training data
      S = {(x_i, y_i)}, m examples
    what we want:
      argmax_{w} P(S|w) = argmax_{w} PRODUCT_{i=1}^{m}{P(y_i | x_i, w)}

      equivalent to 
      argmax_{w} \sum_{i}^{m} log P(y_i | x_i, w)}   